# -*- coding: utf-8 -*-
"""Spam_mail_Predictiom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/vinay2241/OIBSIP/blob/main/Spam_mail_Predictiom.ipynb

OASIS INFOBYTE DATA SCIENCE INTERNSHIP PROJECT.

Task #2: Spam_Mail_Prediction.
"""

#Importing the libraries

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#Uploading the dataset
from google.colab import files
uploaded = files.upload()

#Data Collection and Preprocessing 

#Loading the data from csv to a pandas DataFrame
raw_data = pd.read_csv('/content/spam (2).csv')

print(raw_data)

#replacing the Null/Missing Data(NA,na,NAan) by null string
mail_data = raw_data.where((pd.notnull(raw_data)), '')

#Printing the first 5 rows of the Data frame 
mail_data.head()

#Checking the number of rows and columns in the dataframe 
mail_data.shape

"""Lable Encoding """

#lable Spam-->0 && ham -->1
mail_data.loc[mail_data['Category'] == 'spam', 'Category',] = 0
mail_data.loc[mail_data['Category'] == 'ham', 'Category',] = 1

#Separating the data as text and lable 

X = mail_data['Message']
Y = mail_data['Category']

print(X)

print(Y)

"""Splitting the data into Training part and Testing part"""

#Mentioning the 4 Arrays 
#We are using the 4 parameters i.e, X,Y,test_size & random_state
 
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=3)

print(X.shape) #Total Number of the Message columns 
print(X_train.shape) #80% Data Goes into the training part
print(X_test.shape) #20% Data Goes into the testing part

"""**Feature Extraction**: *Converting/Trasforming the text Data into Numerical data which is gping to help for the Logistic Regression Model*"""



#Feature_extraction
feature_extraction = TfidfVectorizer(min_df = 1, stop_words= 'english', lowercase= True)
#Where TfidVectorizer: Try to go through all the words in the dataset. If the word is get repeated several times then it get some score 
#min_df=1: eans if the score of the particular word is <1 then ignore it
#stopwords: are those words which are repeated multiple times in the data, so we have to ignore
#lowercase:  used for get convert the data which is help for the Model Processing;

#creating an array
X_train_feature = feature_extraction.fit_transform(X_train)
X_test_feature = feature_extraction.transform(X_test)  
#Step 1:Fitting all the data into the Vectorizer
#Step 2:Transfrom all the data into feature vectors they are Nothing but "Numerical Values"

#Converting the Y_train and Y_test as the integers
Y_train = Y_train.astype('int')
Y_test = Y_test.astype('int')

print(X_train)

print(X_train_feature)

"""Training the Model

Logistic Regression
"""

model = LogisticRegression()

#Training the Logistic Regression model with training the data
model.fit(X_train_feature, Y_train)

"""Evaluating the trained data


"""

#Prediction on trained dat

prediction_on_training_data = model.predict(X_train_feature)
accuracy_on_training_data = accuracy_score(Y_train,prediction_on_training_data)

print('Accuracy on training data :', accuracy_on_training_data)

#Prediction on test dat

prediction_on_test_data = model.predict(X_test_feature)
accuracy_on_test_data = accuracy_score(Y_test,prediction_on_test_data)

print('Accuracy on training data :', accuracy_on_test_data)

"""Building the Prediction System"""

input_mail = ["Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."]

#Convert the text to the feature Vector
input_data_feature = feature_extraction.transform(input_mail)

#Making thr Prediction
Prediction= model.predict(input_data_feature)

print(Prediction)

if (Prediction[0] == 1):
  print("Ham Mail")
else:
  print("Spam Mail")

